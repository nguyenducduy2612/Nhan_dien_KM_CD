import cv2
import torch
import numpy as np
import telebot
from datetime import datetime
import os
import time
import requests
import threading
import pickle
from queue import Queue
from ultralytics import YOLO
from deepface import DeepFace

# ================= C·∫§U H√åNH H·ªÜ TH·ªêNG =================
THRESHOLD = 0.3
EMBEDDING_FILE = "embeddings.pkl"
MODEL_NAME = "Facenet512"
FRAME_SIZE = (640, 480)
TELEGRAM_BOT_TOKEN = "7664196118:AAHcWaQcXcwRoSWSPnH6pyUH54LoWofhXQQ"
TELEGRAM_CHAT_ID = "7736232579"
MOTION_THRESHOLD = 1000
ALERT_INTERVAL = 5
VIDEO_RECORD_DURATION = 10

# ================= KH·ªûI T·∫†O TH√ÄNH PH·∫¶N =================
print("[H·ªá th·ªëng] ƒêang kh·ªüi ƒë·ªông c√°c th√†nh ph·∫ßn...")
bot = telebot.TeleBot(TELEGRAM_BOT_TOKEN)
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
yolo = YOLO("yolov8s.pt")
task_queue = Queue()
cap = cv2.VideoCapture(0)
is_recording = False


# ================= QU·∫¢N L√ù TR·∫†NG TH√ÅI =================
class SystemState:
    def __init__(self):
        self.lock = threading.Lock()
        self.last_alert = 0
        self.recognized_name = "ƒêang nh·∫≠n di·ªán..."


system_state = SystemState()

# ================= X·ª¨ L√ù EMBEDDINGS =================
try:
    if os.path.exists(EMBEDDING_FILE):
        with open(EMBEDDING_FILE, 'rb') as f:
            embeddings = pickle.load(f)
        print(f"[Nh·∫≠n di·ªán] ƒê√£ t·∫£i {len(embeddings)} embeddings t·ª´ file")
    else:
        raise FileNotFoundError("Kh√¥ng t√¨m th·∫•y file embeddings!")
except Exception as e:
    print(f"[L·ªói] {str(e)}")
    exit()


# ================= TI·ªÜN √çCH H·ªÜ TH·ªêNG =================
def get_timestamp():
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def create_directory(path):
    try:
        os.makedirs(path, exist_ok=True)
        print(f"[H·ªá th·ªëng] ƒê√£ t·∫°o th∆∞ m·ª•c {path}")
    except Exception as e:
        print(f"[L·ªói] Kh√¥ng th·ªÉ t·∫°o th∆∞ m·ª•c {path}: {str(e)}")


# ================= X·ª¨ L√ù TELEGRAM =================
def send_to_telegram(photo_path, caption="üì∏ ·∫¢nh t·ª´ camera"):
    try:
        with open(photo_path, "rb") as photo:
            requests.post(
                f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendPhoto",
                files={"photo": photo},
                data={"chat_id": TELEGRAM_CHAT_ID, "caption": caption}
            )
        print(f"[Telegram] ƒê√£ g·ª≠i ·∫£nh: {photo_path}")
    except Exception as e:
        print(f"[Telegram] L·ªói g·ª≠i ·∫£nh: {e}")


def send_video_to_telegram(video_path, caption="üé• Video t·ª´ camera"):
    try:
        with open(video_path, "rb") as video:
            requests.post(
                f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendVideo",
                files={"video": video},
                data={"chat_id": TELEGRAM_CHAT_ID, "caption": caption}
            )
        print(f"[Telegram] ƒê√£ g·ª≠i video: {video_path}")
    except Exception as e:
        print(f"[Telegram] L·ªói g·ª≠i video: {e}")


def send_warning():
    try:
        requests.post(
            f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage",
            data={"chat_id": TELEGRAM_CHAT_ID, "text": "‚ö† C·∫£nh b√°o! Ph√°t hi·ªán ng∆∞·ªùi l·∫°!"}
        )
        print("[C·∫£nh b√°o] ƒê√£ g·ª≠i c·∫£nh b√°o ƒë·∫øn Telegram")
    except Exception as e:
        print(f"[C·∫£nh b√°o] L·ªói g·ª≠i c·∫£nh b√°o: {e}")


# ================= X·ª¨ L√ù L·ªÜNH TELEGRAM =================
@bot.message_handler(commands=['photo'])
def capture_photo(message):
    print(f"[Telegram] Nh·∫≠n l·ªánh ch·ª•p ·∫£nh t·ª´ {message.from_user.username}")
    try:
        ret, frame = cap.read()
        if ret:
            create_directory("manual_captures")
            img_name = f"manual_captures/photo_{get_timestamp()}.jpg"
            cv2.imwrite(img_name, frame)
            print(f"[H·ªá th·ªëng] ƒê√£ l∆∞u ·∫£nh th·ªß c√¥ng: {img_name}")
            send_to_telegram(img_name)
            bot.reply_to(message, "üì§ ·∫¢nh ƒë√£ ƒë∆∞·ª£c g·ª≠i!")
        else:
            print("[H·ªá th·ªëng] L·ªói ƒë·ªçc frame t·ª´ camera")
            bot.reply_to(message, "‚ùå Kh√¥ng th·ªÉ ch·ª•p ·∫£nh!")
    except Exception as e:
        print(f"[L·ªói] {str(e)}")
        bot.reply_to(message, f"‚ö† L·ªói h·ªá th·ªëng: {str(e)}")


@bot.message_handler(commands=['record'])
def record_video(message):
    global is_recording
    print(f"[Telegram] Nh·∫≠n l·ªánh ghi video t·ª´ {message.from_user.username}")
    if is_recording:
        print("[H·ªá th·ªëng] ƒêang ghi video kh√°c")
        bot.reply_to(message, "‚è≥ ƒêang trong qu√° tr√¨nh quay video kh√°c...")
        return

    is_recording = True
    bot.reply_to(message, "üé• B·∫Øt ƒë·∫ßu quay video...")

    try:
        create_directory("videos")
        video_name = f"videos/video_{get_timestamp()}.avi"
        fourcc = cv2.VideoWriter_fourcc(*"XVID")
        frame_size = (int(cap.get(3)), int(cap.get(4)))
        out = cv2.VideoWriter(video_name, fourcc, 20.0, frame_size)

        start_time = time.time()
        print(f"[H·ªá th·ªëng] B·∫Øt ƒë·∫ßu ghi video {video_name}")
        while time.time() - start_time < VIDEO_RECORD_DURATION:
            ret, frame = cap.read()
            if ret:
                out.write(frame)

        out.release()
        print(f"[H·ªá th·ªëng] ƒê√£ l∆∞u video: {video_name}")
        send_video_to_telegram(video_name)
        bot.reply_to(message, "üì§ Video ƒë√£ ƒë∆∞·ª£c g·ª≠i!")
    except Exception as e:
        print(f"[L·ªói] Ghi video: {str(e)}")
        bot.reply_to(message, f"‚ö† L·ªói khi quay video: {str(e)}")
    finally:
        is_recording = False


@bot.message_handler(commands=['warning'])
def send_bot_warning(message):
    try:
        print(f"[Telegram] K√≠ch ho·∫°t c·∫£nh b√°o th·ªß c√¥ng b·ªüi {message.from_user.username}")
        send_warning()
        bot.reply_to(message, "‚ö† ƒê√£ g·ª≠i c·∫£nh b√°o!")
    except Exception as e:
        print(f"[L·ªói] {str(e)}")
        bot.reply_to(message, f"‚ö† L·ªói h·ªá th·ªëng: {str(e)}")


# ================= LOGIC GI√ÅM S√ÅT CH√çNH =================
def async_worker():
    print("[H·ªá th·ªëng] Worker b·∫•t ƒë·ªìng b·ªô ƒë√£ kh·ªüi ƒë·ªông")
    while True:
        task = task_queue.get()
        try:
            print(f"[Task] ƒêang x·ª≠ l√Ω task ({task_queue.qsize()} task ch·ªù)")
            task()
        except Exception as e:
            print(f"[L·ªói Task] {str(e)}")
        task_queue.task_done()


threading.Thread(target=async_worker, daemon=True).start()


def process_alert(frame, faces, person_boxes):
    try:
        timestamp = get_timestamp()
        img_path = f"alerts/alert_{timestamp}.jpg"
        create_directory("alerts")

        # V·∫Ω c√°c khung ph√°t hi·ªán
        for (x1, y1, x2, y2) in person_boxes:
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)

        for (x, y, w, h) in faces:
            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

        cv2.imwrite(img_path, frame)
        print(f"[C·∫£nh b√°o] ƒê√£ l∆∞u ·∫£nh c·∫£nh b√°o: {img_path}")

        # X·ª≠ l√Ω nh·∫≠n di·ªán
        best_match = "Ng∆∞·ªùi l·∫°"
        max_sim = 0
        recognition_result = "Ng∆∞·ªùi l·∫°"

        if len(faces) > 0:
            try:
                x, y, w, h = faces[0]
                face_img = frame[y:y + h, x:x + w]

                embedding = DeepFace.represent(
                    img_path=face_img,
                    model_name=MODEL_NAME,
                    enforce_detection=False,
                    detector_backend="opencv"
                )[0]["embedding"]

                # T√¨m khu√¥n m·∫∑t kh·ªõp nh·∫•t
                current_max_sim = 0
                current_best_match = "Ng∆∞·ªùi l·∫°"

                for name, data in embeddings.items():
                    sim = np.dot(embedding, data["embedding"]) / (
                            np.linalg.norm(embedding) * np.linalg.norm(data["embedding"]))
                    if sim > current_max_sim and sim > THRESHOLD:
                        current_max_sim = sim
                        current_best_match = name

                best_match = current_best_match
                max_sim = current_max_sim

                recognition_result = (
                    f"{best_match} ({max_sim:.2f})"
                    if best_match != "Ng∆∞·ªùi l·∫°"
                    else "Ng∆∞·ªùi l·∫°"
                )
                print(f"[Nh·∫≠n di·ªán] K·∫øt qu·∫£: {recognition_result}")

            except Exception as e:
                print(f"[L·ªói Nh·∫≠n di·ªán] {str(e)}")
                recognition_result = "L·ªói nh·∫≠n di·ªán"

        # T·∫°o caption th√¥ng b√°o
        if best_match != "Ng∆∞·ªùi l·∫°":
            caption = f"‚úÖ Nh·∫≠n di·ªán: {best_match} (ƒê·ªô ch√≠nh x√°c: {max_sim:.2f})"
        else:
            caption = "‚ö†Ô∏è C·∫£nh b√°o! Ph√°t hi·ªán ng∆∞·ªùi l·∫°"

        send_to_telegram(img_path, caption)

    except Exception as e:
        print(f"[L·ªói X·ª≠ l√Ω C·∫£nh b√°o] {str(e)}")

def camera_loop():
    print("[H·ªá th·ªëng] Lu·ªìng camera ƒë√£ kh·ªüi ƒë·ªông")
    prev_frame = None

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            print("[L·ªói] Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c frame t·ª´ camera")
            break

        # Ph√°t hi·ªán chuy·ªÉn ƒë·ªông
        motion_detected = False
        if prev_frame is not None:
            gray1 = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
            gray2 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            diff = cv2.absdiff(gray1, gray2)
            _, thresh = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)
            contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

            for c in contours:
                if cv2.contourArea(c) > MOTION_THRESHOLD:
                    motion_detected = True
                    break

        if motion_detected:
            print("[Ph√°t hi·ªán] Ph√°t hi·ªán chuy·ªÉn ƒë·ªông")
            results = yolo(frame, verbose=False)
            person_boxes = []

            for r in results:
                for box in r.boxes:
                    if int(box.cls) == 0:
                        x1, y1, x2, y2 = map(int, box.xyxy[0])
                        person_boxes.append((x1, y1, x2, y2))

            if person_boxes:
                print(f"[Ph√°t hi·ªán] T√¨m th·∫•y {len(person_boxes)} ng∆∞·ªùi")
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                faces = face_cascade.detectMultiScale(gray, 1.1, 5)
                print(f"[Ph√°t hi·ªán] T√¨m th·∫•y {len(faces)} khu√¥n m·∫∑t")

                current_time = time.time()
                if current_time - system_state.last_alert > ALERT_INTERVAL:
                    task_queue.put(lambda: process_alert(frame.copy(), faces, person_boxes))
                    with system_state.lock:
                        system_state.last_alert = current_time
                    print("[C·∫£nh b√°o] ƒê√£ th√™m task c·∫£nh b√°o v√†o h√†ng ƒë·ª£i")

        prev_frame = frame.copy()
        cv2.imshow("Security System", frame)

        if cv2.waitKey(1) == ord('q'):
            print("[H·ªá th·ªëng] ƒêang t·∫Øt camera...")
            break

    cap.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    print("[H·ªá th·ªëng] ƒêang kh·ªüi t·∫°o th∆∞ m·ª•c...")
    create_directory("alerts")
    create_directory("manual_captures")
    create_directory("videos")

    print("[H·ªá th·ªëng] Kh·ªüi ƒë·ªông th√†nh c√¥ng!")
    threading.Thread(target=camera_loop, daemon=True).start()
    bot.polling(none_stop=True)