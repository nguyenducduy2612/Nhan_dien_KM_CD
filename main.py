import cv2
import torch
import numpy as np
import telebot
from datetime import datetime
import os
import time
import requests
import threading
import pickle
from queue import Queue
from ultralytics import YOLO
from deepface import DeepFace

# ================= C·∫§U H√åNH H·ªÜ TH·ªêNG =================
THRESHOLD = 0.3
EMBEDDING_FILE = "embeddings.pkl"
MODEL_NAME = "Facenet512"
FRAME_SIZE = (640, 480)
TELEGRAM_BOT_TOKEN = "7664196118:AAHcWaQcXcwRoSWSPnH6pyUH54LoWofhXQQ"
TELEGRAM_CHAT_ID = "7736232579"
MOTION_THRESHOLD = 1000
ALERT_INTERVAL = 5
VIDEO_RECORD_DURATION = 10
BUZZER_API_URL = "http://192.168.137.48"  # Thay b·∫±ng IP th·ª±c t·∫ø c·ªßa ESP8266
BUZZER_DURATION = 10  # Th·ªùi gian buzzer k√™u (gi√¢y)

# ================= KH·ªûI T·∫†O TH√ÄNH PH·∫¶N =================
print("[H·ªá th·ªëng] ƒêang kh·ªüi ƒë·ªông c√°c th√†nh ph·∫ßn...")
bot = telebot.TeleBot(TELEGRAM_BOT_TOKEN)
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
yolo = YOLO("yolov8s.pt")
task_queue = Queue()
cap = cv2.VideoCapture(0)
is_recording = False

# ================= QU·∫¢N L√ù TR·∫†NG TH√ÅI =================
class SystemState:
    def __init__(self):
        self.lock = threading.Lock()
        self.last_alert = 0
        self.recognized_name = "ƒêang nh·∫≠n di·ªán..."


system_state = SystemState()

# ================= X·ª¨ L√ù EMBEDDINGS =================
try:
    if os.path.exists(EMBEDDING_FILE):
        with open(EMBEDDING_FILE, 'rb') as f:
            embeddings = pickle.load(f)
        print(f"[Nh·∫≠n di·ªán] ƒê√£ t·∫£i {len(embeddings)} embeddings t·ª´ file")
    else:
        raise FileNotFoundError("Kh√¥ng t√¨m th·∫•y file embeddings!")
except Exception as e:
    print(f"[L·ªói] {str(e)}")
    exit()

# ================= H√ÄM ƒêI·ªÄU KHI·ªÇN BUZZER =================
def trigger_buzzer():
    """G·ª≠i y√™u c·∫ßu HTTP ƒë·∫øn ESP8266 ƒë·ªÉ b·∫≠t buzzer trong 10 gi√¢y"""
    try:
        print("[Buzzer] üö® G·ª≠i y√™u c·∫ßu b·∫≠t buzzer...")
        # G·ª≠i y√™u c·∫ßu b·∫≠t buzzer
        response = requests.get(f"{BUZZER_API_URL}/buzzer/on", timeout=5)
        if response.status_code == 200:
            print("[Buzzer] ‚úÖ Buzzer ƒë√£ ƒë∆∞·ª£c b·∫≠t")
        else:
            print(f"[Buzzer] ‚ö† L·ªói khi b·∫≠t buzzer: {response.status_code}")

        # Ch·ªù 10 gi√¢y
        time.sleep(BUZZER_DURATION)

        # G·ª≠i y√™u c·∫ßu t·∫Øt buzzer
        response = requests.get(f"{BUZZER_API_URL}/buzzer/off", timeout=5)
        if response.status_code == 200:
            print("[Buzzer] ‚úÖ Buzzer ƒë√£ t·∫Øt")
        else:
            print(f"[Buzzer] ‚ö† L·ªói khi t·∫Øt buzzer: {response.status_code}")
    except requests.RequestException as e:
        print(f"[L·ªói Buzzer] ‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn ESP8266: {str(e)}")

# ================= TI·ªÜN √çCH H·ªÜ TH·ªêNG =================
def get_timestamp():
    return datetime.now().strftime("%Y%m%d_%H%M%S")

def create_directory(path):
    try:
        os.makedirs(path, exist_ok=True)
        print(f"[H·ªá th·ªëng] ƒê√£ t·∫°o th∆∞ m·ª•c {path}")
    except Exception as e:
        print(f"[L·ªói] Kh√¥ng th·ªÉ t·∫°o th∆∞ m·ª•c {path}: {str(e)}")

# ================= X·ª¨ L√ù TELEGRAM =================
def send_to_telegram(photo_path, caption="üì∏ ·∫¢nh t·ª´ camera"):
    try:
        with open(photo_path, "rb") as photo:
            requests.post(
                f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendPhoto",
                files={"photo": photo},
                data={"chat_id": TELEGRAM_CHAT_ID, "caption": caption}
            )
        print(f"[Telegram] ƒê√£ g·ª≠i ·∫£nh: {photo_path}")
    except Exception as e:
        print(f"[Telegram] L·ªói g·ª≠i ·∫£nh: {e}")

def send_video_to_telegram(video_path, caption="üé• Video t·ª´ camera"):
    try:
        with open(video_path, "rb") as video:
            requests.post(
                f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendVideo",
                files={"video": video},
                data={"chat_id": TELEGRAM_CHAT_ID, "caption": caption}
            )
        print(f"[Telegram] ƒê√£ g·ª≠i video: {video_path}")
    except Exception as e:
        print(f"[Telegram] L·ªói g·ª≠i video: {e}")

def send_warning():
    try:
        requests.post(
            f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage",
            data={"chat_id": TELEGRAM_CHAT_ID, "text": "‚ö† C·∫£nh b√°o! Ph√°t hi·ªán ng∆∞·ªùi l·∫°!"}
        )
        print("[C·∫£nh b√°o] ƒê√£ g·ª≠i c·∫£nh b√°o ƒë·∫øn Telegram")
    except Exception as e:
        print(f"[C·∫£nh b√°o] L·ªói g·ª≠i c·∫£nh b√°o: {e}")

# ================= X·ª¨ L√ù L·ªÜNH TELEGRAM =================
@bot.message_handler(commands=['photo'])
def capture_photo(message):
    print(f"[Telegram] Nh·∫≠n l·ªánh ch·ª•p ·∫£nh t·ª´ {message.from_user.username}")
    try:
        ret, frame = cap.read()
        if ret:
            create_directory("manual_captures")
            img_name = f"manual_captures/photo_{get_timestamp()}.jpg"
            cv2.imwrite(img_name, frame)
            print(f"[H·ªá th·ªëng] ƒê√£ l∆∞u ·∫£nh th·ªß c√¥ng: {img_name}")
            send_to_telegram(img_name)
            bot.reply_to(message, "üì§ ·∫¢nh ƒë√£ ƒë∆∞·ª£c g·ª≠i!")
        else:
            print("[H·ªá th·ªëng] L·ªói ƒë·ªçc frame t·ª´ camera")
            bot.reply_to(message, "‚ùå Kh√¥ng th·ªÉ ch·ª•p ·∫£nh!")
    except Exception as e:
        print(f"[L·ªói] {str(e)}")
        bot.reply_to(message, f"‚ö† L·ªói h·ªá th·ªëng: {str(e)}")

@bot.message_handler(commands=['record'])
def record_video(message):
    global is_recording
    print(f"[Telegram] Nh·∫≠n l·ªánh ghi video t·ª´ {message.from_user.username}")
    if is_recording:
        print("[H·ªá th·ªëng] ƒêang ghi video kh√°c")
        bot.reply_to(message, "‚è≥ ƒêang trong qu√° tr√¨nh quay video kh√°c...")
        return

    is_recording = True
    bot.reply_to(message, "üé• B·∫Øt ƒë·∫ßu quay video...")

    try:
        create_directory("videos")
        video_name = f"videos/video_{get_timestamp()}.avi"
        fourcc = cv2.VideoWriter_fourcc(*"XVID")
        frame_size = (int(cap.get(3)), int(cap.get(4)))
        out = cv2.VideoWriter(video_name, fourcc, 20.0, frame_size)

        start_time = time.time()
        print(f"[H·ªá th·ªëng] B·∫Øt ƒë·∫ßu ghi video {video_name}")
        while time.time() - start_time < VIDEO_RECORD_DURATION:
            ret, frame = cap.read()
            if ret:
                out.write(frame)

        out.release()
        print(f"[H·ªá th·ªëng] ƒê√£ l∆∞u video: {video_name}")
        send_video_to_telegram(video_name)
        bot.reply_to(message, "üì§ Video ƒë√£ ƒë∆∞·ª£c g·ª≠i!")
    except Exception as e:
        print(f"[L·ªói] Ghi video: {str(e)}")
        bot.reply_to(message, f"‚ö† L·ªói khi quay video: {str(e)}")
    finally:
        is_recording = False

@bot.message_handler(commands=['warning'])
def send_bot_warning(message):
    try:
        print(f"[Telegram] K√≠ch ho·∫°t c·∫£nh b√°o th·ªß c√¥ng b·ªüi {message.from_user.username}")
        send_warning()
        bot.reply_to(message, "‚ö† ƒê√£ g·ª≠i c·∫£nh b√°o!")
    except Exception as e:
        print(f"[L·ªói] {str(e)}")
        bot.reply_to(message, f"‚ö† L·ªói h·ªá th·ªëng: {str(e)}")

# ================= LOGIC GI√ÅM S√ÅT CH√çNH =================
def async_worker():
    print("[H·ªá th·ªëng] Worker b·∫•t ƒë·ªìng b·ªô ƒë√£ kh·ªüi ƒë·ªông")
    while True:
        task = task_queue.get()
        try:
            print(f"[Task] ƒêang x·ª≠ l√Ω task ({task_queue.qsize()} task ch·ªù)")
            task()
        except Exception as e:
            print(f"[L·ªói Task] {str(e)}")
        task_queue.task_done()

threading.Thread(target=async_worker, daemon=True).start()

def process_alert(frame, faces, person_boxes):
    try:
        timestamp = get_timestamp()
        img_path = f"alerts/alert_{timestamp}.jpg"
        create_directory("alerts")

        # V·∫Ω c√°c khung ph√°t hi·ªán
        for (x1, y1, x2, y2) in person_boxes:
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)

        for (x, y, w, h) in faces:
            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

        # X·ª≠ l√Ω nh·∫≠n di·ªán
        best_match = "Ng∆∞·ªùi l·∫°"
        max_sim = 0
        recognition_result = ""
        caption = ""

        if len(person_boxes) > 0:
            if len(faces) > 0:
                try:
                    x, y, w, h = faces[0]
                    face_img = frame[y:y + h, x:x + w]

                    embedding = DeepFace.represent(
                        img_path=face_img,
                        model_name=MODEL_NAME,
                        enforce_detection=False,
                        detector_backend="opencv"
                    )[0]["embedding"]

                    # T√¨m khu√¥n m·∫∑t kh·ªõp nh·∫•t
                    current_max_sim = 0
                    current_best_match = "Ng∆∞·ªùi l·∫°"

                    for name, data in embeddings.items():
                        sim = np.dot(embedding, data["embedding"]) / (
                                np.linalg.norm(embedding) * np.linalg.norm(data["embedding"]))
                        if sim > current_max_sim and sim > THRESHOLD:
                            current_max_sim = sim
                            current_best_match = os.path.basename(os.path.dirname(name))

                    best_match = current_best_match
                    max_sim = current_max_sim

                    if best_match != "Ng∆∞·ªùi l·∫°":
                        recognition_result = f"{best_match} ({max_sim:.2f})"
                        caption = f"‚úÖ Nh·∫≠n di·ªán: {best_match} (ƒê·ªô ch√≠nh x√°c: {max_sim:.2f})"
                        print(f"[Nh·∫≠n di·ªán] ‚úÖ K·∫øt qu·∫£: {recognition_result}")
                    else:
                        recognition_result = "Khu√¥n m·∫∑t kh√¥ng kh·ªõp"
                        caption = "‚ö†Ô∏è C·∫£nh b√°o! Khu√¥n m·∫∑t kh√¥ng kh·ªõp v·ªõi database"
                        print(f"[Nh·∫≠n di·ªán] ‚ö†Ô∏è K·∫øt qu·∫£: Khu√¥n m·∫∑t kh√¥ng kh·ªõp")
                        # K√≠ch ho·∫°t buzzer khi ph√°t hi·ªán ng∆∞·ªùi l·∫°
                        threading.Thread(target=trigger_buzzer, daemon=True).start()
                        send_warning()

                except Exception as e:
                    print(f"[L·ªói Nh·∫≠n di·ªán] ‚ùå {str(e)}")
                    recognition_result = "L·ªói nh·∫≠n di·ªán khu√¥n m·∫∑t"
                    caption = "‚ùå L·ªói! Kh√¥ng th·ªÉ nh·∫≠n di·ªán khu√¥n m·∫∑t"
            else:
                recognition_result = "Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t"
                caption = "üîî Ph√°t hi·ªán ng∆∞·ªùi nh∆∞ng kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c khu√¥n m·∫∑t"
                print(f"[Nh·∫≠n di·ªán] üîî Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t")
                # K√≠ch ho·∫°t buzzer khi kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c khu√¥n m·∫∑t
                threading.Thread(target=trigger_buzzer, daemon=True).start()
                send_warning()
        else:
            recognition_result = "Kh√¥ng c√≥ ng∆∞·ªùi"
            caption = "‚ÑπÔ∏è Kh√¥ng ph√°t hi·ªán ng∆∞·ªùi trong khung h√¨nh"
            print(f"[Nh·∫≠n di·ªán] ‚ÑπÔ∏è Kh√¥ng c√≥ ng∆∞·ªùi")

        # Th√™m vƒÉn b·∫£n l√™n ·∫£nh
        font = cv2.FONT_HERSHEY_DUPLEX
        font_scale = 0.8
        font_color = (255, 255, 255)  # M√†u tr·∫Øng
        thickness = 2
        line_spacing = 30

        time_text = f"Time: {timestamp}"
        status_text = f"Status: {recognition_result}"

        position_time = (10, 30)
        position_status = (10, 30 + line_spacing)

        text_size_time, _ = cv2.getTextSize(time_text, font, font_scale, thickness)
        text_size_status, _ = cv2.getTextSize(status_text, font, font_scale, thickness)
        background_top_left = (5, 5)
        background_bottom_right = (
            max(text_size_time[0], text_size_status[0]) + 15,
            30 + 2 * line_spacing
        )
        cv2.rectangle(frame, background_top_left, background_bottom_right, (0, 0, 0), -1)

        cv2.putText(frame, time_text, position_time, font, font_scale, font_color, thickness)
        cv2.putText(frame, status_text, position_status, font, font_scale, font_color, thickness)

        # L∆∞u ·∫£nh
        cv2.imwrite(img_path, frame)
        print(f"[C·∫£nh b√°o] üíæ ƒê√£ l∆∞u ·∫£nh c·∫£nh b√°o: {img_path}")

        # G·ª≠i th√¥ng b√°o t·ªõi Telegram
        send_to_telegram(img_path, caption)

    except Exception as e:
        print(f"[L·ªói X·ª≠ l√Ω C·∫£nh b√°o] ‚ùå {str(e)}")

def camera_loop():
    print("[H·ªá th·ªëng] ‚úÖ Lu·ªìng camera ƒë√£ kh·ªüi ƒë·ªông")
    prev_frame = None

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            print("[L·ªói] ‚ùå Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c frame t·ª´ camera")
            break

        # Ph√°t hi·ªán chuy·ªÉn ƒë·ªông
        motion_detected = False
        if prev_frame is not None:
            gray1 = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
            gray2 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            diff = cv2.absdiff(gray1, gray2)
            _, thresh = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)
            contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

            for c in contours:
                if cv2.contourArea(c) > MOTION_THRESHOLD:
                    motion_detected = True
                    break

        if motion_detected:
            print("[Ph√°t hi·ªán] üîî Ph√°t hi·ªán chuy·ªÉn ƒë·ªông")
            results = yolo(frame, verbose=False)
            person_boxes = []

            for r in results:
                for box in r.boxes:
                    if int(box.cls) == 0:
                        x1, y1, x2, y2 = map(int, box.xyxy[0])
                        person_boxes.append((x1, y1, x2, y2))

            if person_boxes:
                print(f"[Ph√°t hi·ªán] üë§ T√¨m th·∫•y {len(person_boxes)} ng∆∞·ªùi")
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                faces = face_cascade.detectMultiScale(gray, 1.1, 5)

                current_time = time.time()
                if current_time - system_state.last_alert > ALERT_INTERVAL:
                    task_queue.put(lambda: process_alert(frame.copy(), faces, person_boxes))
                    with system_state.lock:
                        system_state.last_alert = current_time
                    print("[C·∫£nh b√°o] üö® ƒê√£ th√™m task c·∫£nh b√°o v√†o h√†ng ƒë·ª£i")

        prev_frame = frame.copy()
        cv2.imshow("Security System", frame)

        if cv2.waitKey(1) == ord('q'):
            print("[H·ªá th·ªëng] ‚èπÔ∏è ƒêang t·∫Øt camera...")
            break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    print("[H·ªá th·ªëng] ƒêang kh·ªüi t·∫°o th∆∞ m·ª•c...")
    create_directory("alerts")
    create_directory("manual_captures")
    create_directory("videos")

    print("[H·ªá th·ªëng] Kh·ªüi ƒë·ªông th√†nh c√¥ng!")
    threading.Thread(target=camera_loop, daemon=True).start()
    bot.polling(none_stop=True)